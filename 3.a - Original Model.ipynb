{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf73aae",
   "metadata": {},
   "source": [
    "# Unpickle X & Y; Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374852aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "X_combined, y_combined = pickle.load(open('Xy_combined.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934e903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.05, random_state=42)\n",
    "X_train_combined, X_va_combined, y_train_combined, y_va_combined = train_test_split(X_train_combined, y_train_combined, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4eb66",
   "metadata": {},
   "source": [
    "# Baseline without Sequential Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d8e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE with route features: 213.83 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# get the baseline mae again\n",
    "model_combined = LinearRegression()\n",
    "model_combined.fit(X_train_combined, y_train_combined)\n",
    "y_pred_combined = model_combined.predict(X_test_combined)\n",
    "mae_combined = mean_absolute_error(y_test_combined, y_pred_combined)\n",
    "print(f\"Baseline MAE with route features: {mae_combined:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6812667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with Route Features MAE: 197.36950778035285\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# Train XGBoost model with route features\n",
    "xgb_model_combined = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "xgb_model_combined.fit(X_train_combined, y_train_combined)\n",
    "y_pred_combined = xgb_model_combined.predict(X_test_combined)\n",
    "print(\"XGBoost with Route Features MAE:\", mean_absolute_error(y_test_combined, y_pred_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e34bf6",
   "metadata": {},
   "source": [
    "# Transformer - Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "907a10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "padded_routes = pickle.load(open('padded_routes.pkl', 'rb'))\n",
    "encoded_routes = pickle.load(open('encoded_routes.pkl', 'rb'))\n",
    "attention_mask = torch.tensor([\n",
    "    [0]+ [1]*len(r) + [0]*(padded_routes.shape[1] - len(r)) for r in encoded_routes\n",
    "])  # (B, L+1) due to the added start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1059f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16470"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6936b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalars used: 22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Sequence input: (B, N, 2) where B=batch, N=#points, 2=(lat, lon)\n",
    "seq_input = torch.tensor(padded_routes, dtype=torch.float32)  # shape: (B, N, 2)\n",
    "\n",
    "print(f\"Scalars used: {len(X_combined.columns)}\")# Scalar input: (B, F)\n",
    "scalar_input = torch.tensor(X_combined.values, dtype=torch.float32)  # shape: (B, F)\n",
    "\n",
    "# Output: ETA in seconds\n",
    "eta_target = torch.tensor(y_combined, dtype=torch.float32)  # shape: (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec1139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take 5% of seq,scalar, and eta for validation\n",
    "seq_input, seq_test, scalar_input, scalar_test, eta_target, eta_test, attention_mask, attention_mask_test = train_test_split(\n",
    "    seq_input, scalar_input, eta_target, attention_mask, test_size=0.05, random_state=42)\n",
    "seq_input, seq_va, scalar_input, scalar_va, eta_target, eta_va, attention_mask, attention_mask_va = train_test_split(\n",
    "    seq_input, scalar_input, eta_target, attention_mask, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f48402e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14863, 114, 2]),\n",
       " torch.Size([14863, 22]),\n",
       " torch.Size([14863]),\n",
       " torch.Size([14863, 115]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_input.shape, scalar_input.shape, eta_target.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e3542",
   "metadata": {},
   "source": [
    "# Transformer - Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8830e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ETA_Transformer(nn.Module):\n",
    "    def __init__(self, seq_dim=2, scalar_dim=22, d_model=256, nhead=8, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(seq_dim, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(500, d_model))  # max 500 points\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(dim_feedforward=1024,d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.scalar_proj = nn.Linear(scalar_dim, d_model)\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(16, 1)  # Final output for ETA prediction\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "\n",
    "\n",
    "    def forward(self, route_seq, scalar_feats, attention_mask):\n",
    "        \"\"\"\n",
    "        route_seq: (B, N, 2)\n",
    "        scalar_feats: (B, F)\n",
    "        \"\"\"\n",
    "        B, N, _ = route_seq.size()\n",
    "        x = self.input_proj(route_seq)  # (B, N, d_model)\n",
    "\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, 1, -1)  # (B, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, d_model)\n",
    "\n",
    "        x = x + self.pos_encoding[:N+1]  # add positional encoding, N+1 because of [CLS] token\n",
    "\n",
    "        x = self.transformer(x, src_key_padding_mask=(attention_mask == 0))  # (B, N, d_model)\n",
    "        route_embedding = x[:, 0, :]  # [CLS] token output\n",
    "        \n",
    "        mask = attention_mask.unsqueeze(-1)  # (B, L, 1)\n",
    "        masked_x = x * mask  # zero out padding\n",
    "        route_embedding = masked_x.sum(1) / mask.sum(1)  # (B, d_model)\n",
    "        #route_embedding = x.mean(dim=1)  # (B, d_model)\n",
    "\n",
    "        scalar_embedding = self.scalar_proj(scalar_feats)  # (B, d_model)\n",
    "        combined = torch.cat([route_embedding, scalar_embedding], dim=1)  # (B, 2*d_model)\n",
    "\n",
    "        eta_pred = self.output_head(combined).squeeze(-1)  # (B,)\n",
    "        return eta_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd4f1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ETA_Transformer()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.L1Loss()\n",
    "epochs = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc61884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 679.38\n",
      "Validation Loss: 616.64\n",
      "Epoch 2, Loss: 618.92\n",
      "Validation Loss: 554.95\n",
      "Epoch 3, Loss: 533.67\n",
      "Validation Loss: 496.09\n",
      "Epoch 4, Loss: 504.80\n",
      "Validation Loss: 441.51\n",
      "Epoch 5, Loss: 429.88\n",
      "Validation Loss: 393.88\n",
      "Epoch 6, Loss: 380.67\n",
      "Validation Loss: 353.29\n",
      "Epoch 7, Loss: 346.31\n",
      "Validation Loss: 319.40\n",
      "Epoch 8, Loss: 322.05\n",
      "Validation Loss: 292.03\n",
      "Epoch 9, Loss: 276.48\n",
      "Validation Loss: 272.67\n",
      "Epoch 10, Loss: 258.75\n",
      "Validation Loss: 261.44\n",
      "Epoch 11, Loss: 262.74\n",
      "Validation Loss: 256.51\n",
      "Epoch 12, Loss: 238.18\n",
      "Validation Loss: 256.16\n",
      "Epoch 13, Loss: 251.34\n",
      "Validation Loss: 257.98\n",
      "Epoch 14, Loss: 241.94\n",
      "Validation Loss: 261.53\n",
      "Epoch 15, Loss: 262.32\n",
      "Validation Loss: 266.45\n",
      "Epoch 16, Loss: 254.66\n",
      "Validation Loss: 270.49\n",
      "Epoch 17, Loss: 265.38\n",
      "Validation Loss: 272.82\n",
      "Epoch 18, Loss: 263.27\n",
      "Validation Loss: 273.71\n",
      "Epoch 19, Loss: 286.95\n",
      "Validation Loss: 272.34\n",
      "Epoch 20, Loss: 242.86\n",
      "Validation Loss: 269.49\n",
      "Epoch 21, Loss: 246.91\n",
      "Validation Loss: 265.19\n",
      "Epoch 22, Loss: 223.16\n",
      "Validation Loss: 260.23\n",
      "Epoch 23, Loss: 242.25\n",
      "Validation Loss: 255.10\n",
      "Epoch 24, Loss: 239.21\n",
      "Validation Loss: 250.31\n",
      "Epoch 25, Loss: 249.34\n",
      "Validation Loss: 245.81\n",
      "Epoch 26, Loss: 250.29\n",
      "Validation Loss: 241.46\n",
      "Epoch 27, Loss: 239.86\n",
      "Validation Loss: 237.88\n",
      "Epoch 28, Loss: 228.42\n",
      "Validation Loss: 235.03\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    #sample a 5% batch\n",
    "    batch_size = int(0.05 * seq_input.shape[0])\n",
    "    indices = torch.randperm(seq_input.shape[0])[:batch_size]\n",
    "    seq_input_batch = seq_input[indices]\n",
    "    scalar_input_batch = scalar_input[indices]\n",
    "    eta_target_batch = eta_target[indices]\n",
    "    attention_mask_batch = attention_mask[indices]\n",
    "\n",
    "    pred = model(seq_input_batch, scalar_input_batch,attention_mask_batch)\n",
    "    loss = loss_fn(pred, eta_target_batch)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.2f}\")\n",
    "    # print validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_test = model(seq_test, scalar_test, attention_mask_test)\n",
    "        test_loss = loss_fn(pred_test, eta_test)\n",
    "        print(f\"Validation Loss: {test_loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e4baeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import pickle\n",
    "pickle.dump(model,open('trained_transformer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f039a3d",
   "metadata": {},
   "source": [
    "# Performance eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5fb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 228.75\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred_va = model(seq_va, scalar_va, attention_mask_va)\n",
    "    va_loss = loss_fn(pred_va, eta_va)\n",
    "    print(f\"Test Loss: {va_loss.item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
