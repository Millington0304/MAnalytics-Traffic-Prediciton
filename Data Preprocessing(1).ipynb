{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c308280a",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "**Objective**  \n",
    "Unify raw trip datasets, clean and standardize columns, engineer time features, map pickup coordinates to taxi zones, remove implausible records, and export compact artifacts for downstream ML modeling.\n",
    "\n",
    "**What this notebook does**  \n",
    "- Loads multiple monthly files (Parquet) and optionally an extra CSV of “new” trips  \n",
    "- Normalizes dtypes (datetimes, categoricals, nullable integers), handles missing values  \n",
    "- Engineers features (trip duration in minutes; hour/day features)  \n",
    "- Performs a spatial join from pickup lon/lat → taxi zones (WGS84 / EPSG:4326)  \n",
    "- Filters invalid trips (e.g., nonpositive or extreme durations)  \n",
    "- Saves cleaned data as fast-loading `.npz` artifacts\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- **Monthly Parquet files**: `fhv_tripdata_YYYY-MM.parquet`  \n",
    "  - Minimum expected columns:  \n",
    "    - `pickup_datetime` `datetime64[ns]`  \n",
    "    - `dropoff_datetime` `datetime64[ns]`  \n",
    "    - `PUlocationID`, `DOlocationID` `Int64` (nullable)  \n",
    "    - Optional: `pickup_longitude`, `pickup_latitude` `float` (WGS84)\n",
    "    - Optional: `Affiliated_base_number` `category/str`\n",
    "- **Taxi zones layer**: polygons in GeoPackage/GeoJSON/Shapefile loaded as a `GeoDataFrame` named `taxi_zones` with **CRS = EPSG:4326**\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- **`npz/combined_trips_clean.npz`**  \n",
    "  - Cleaned consolidated trips as a table-equivalent (NumPy `.npz`) containing at least:  \n",
    "    - `pickup_datetime` `datetime64[ns]`  \n",
    "    - `dropoff_datetime` `datetime64[ns]`  \n",
    "    - `trip_duration_minutes` `float` (minutes)  \n",
    "    - `PUlocationID`, `DOlocationID` `Int64`  \n",
    "    - Optional engineered: `hour` `int` (0–23), `day_of_week` `int` (0–6), `day_of_month` `int` (1–31)  \n",
    "- **`npz/new_trips_with_zones.npz`**\n",
    "  - “New trips” with appended zone attributes (e.g., `LocationID`, `zone`) and the same engineered features as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bef2a",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c133d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading fhv_tripdata_2025-02.parquet...\n",
      "Successfully read fhv_tripdata_2025-02.parquet. Shape: (1578722, 7)\n",
      "Reading fhv_tripdata_2021-05.parquet...\n",
      "Successfully read fhv_tripdata_2021-05.parquet. Shape: (1263660, 7)\n",
      "Reading fhv_tripdata_2022-09.parquet...\n",
      "Successfully read fhv_tripdata_2022-09.parquet. Shape: (1160493, 7)\n",
      "Reading fhv_tripdata_2024-04.parquet...\n",
      "Successfully read fhv_tripdata_2024-04.parquet. Shape: (1444626, 7)\n",
      "Reading fhv_tripdata_2023-06.parquet...\n",
      "Successfully read fhv_tripdata_2023-06.parquet. Shape: (1219445, 7)\n",
      "Reading fhv_tripdata_2022-10.parquet...\n",
      "Successfully read fhv_tripdata_2022-10.parquet. Shape: (1174988, 7)\n",
      "Reading fhv_tripdata_2023-07.parquet...\n",
      "Successfully read fhv_tripdata_2023-07.parquet. Shape: (1370843, 7)\n",
      "Reading fhv_tripdata_2022-11.parquet...\n",
      "Successfully read fhv_tripdata_2022-11.parquet. Shape: (1106084, 7)\n",
      "Reading fhv_tripdata_2022-01.parquet...\n",
      "Successfully read fhv_tripdata_2022-01.parquet. Shape: (1143691, 7)\n",
      "Reading fhv_tripdata_2025-03.parquet...\n",
      "Successfully read fhv_tripdata_2025-03.parquet. Shape: (2182992, 7)\n",
      "Reading fhv_tripdata_2021-04.parquet...\n",
      "Successfully read fhv_tripdata_2021-04.parquet. Shape: (1267018, 7)\n",
      "Reading fhv_tripdata_2022-08.parquet...\n",
      "Successfully read fhv_tripdata_2022-08.parquet. Shape: (1151155, 7)\n",
      "Reading fhv_tripdata_2024-05.parquet...\n",
      "Successfully read fhv_tripdata_2024-05.parquet. Shape: (1352502, 7)\n",
      "Reading fhv_tripdata_2023-05.parquet...\n",
      "Successfully read fhv_tripdata_2023-05.parquet. Shape: (1385826, 7)\n",
      "Reading fhv_tripdata_2022-03.parquet...\n",
      "Successfully read fhv_tripdata_2022-03.parquet. Shape: (1380816, 7)\n",
      "Reading fhv_tripdata_2025-01.parquet...\n",
      "Successfully read fhv_tripdata_2025-01.parquet. Shape: (1898108, 7)\n",
      "Reading fhv_tripdata_2024-07.parquet...\n",
      "Successfully read fhv_tripdata_2024-07.parquet. Shape: (1382739, 7)\n",
      "Reading fhv_tripdata_2021-06.parquet...\n",
      "Successfully read fhv_tripdata_2021-06.parquet. Shape: (1311346, 7)\n",
      "Reading fhv_tripdata_2024-06.parquet...\n",
      "Successfully read fhv_tripdata_2024-06.parquet. Shape: (1386539, 7)\n",
      "Reading fhv_tripdata_2021-07.parquet...\n",
      "Successfully read fhv_tripdata_2021-07.parquet. Shape: (1240014, 7)\n",
      "Reading fhv_tripdata_2023-04.parquet...\n",
      "Successfully read fhv_tripdata_2023-04.parquet. Shape: (1246479, 7)\n",
      "Reading fhv_tripdata_2022-12.parquet...\n",
      "Successfully read fhv_tripdata_2022-12.parquet. Shape: (1285443, 7)\n",
      "Reading fhv_tripdata_2022-02.parquet...\n",
      "Successfully read fhv_tripdata_2022-02.parquet. Shape: (1251504, 7)\n",
      "Reading fhv_tripdata_2022-07.parquet...\n",
      "Successfully read fhv_tripdata_2022-07.parquet. Shape: (1159579, 7)\n",
      "Reading fhv_tripdata_2023-01.parquet...\n",
      "Successfully read fhv_tripdata_2023-01.parquet. Shape: (1114320, 7)\n",
      "Reading fhv_tripdata_2023-11.parquet...\n",
      "Successfully read fhv_tripdata_2023-11.parquet. Shape: (1343846, 7)\n",
      "Reading fhv_tripdata_2024-03.parquet...\n",
      "Successfully read fhv_tripdata_2024-03.parquet. Shape: (1469352, 7)\n",
      "Reading fhv_tripdata_2021-12.parquet...\n",
      "Successfully read fhv_tripdata_2021-12.parquet. Shape: (1339973, 7)\n",
      "Reading fhv_tripdata_2021-02.parquet...\n",
      "Successfully read fhv_tripdata_2021-02.parquet. Shape: (1037692, 7)\n",
      "Reading fhv_tripdata_2023-08.parquet...\n",
      "Successfully read fhv_tripdata_2023-08.parquet. Shape: (1440352, 7)\n",
      "Reading fhv_tripdata_2024-12.parquet...\n",
      "Successfully read fhv_tripdata_2024-12.parquet. Shape: (1913200, 7)\n",
      "Reading fhv_tripdata_2024-02.parquet...\n",
      "Successfully read fhv_tripdata_2024-02.parquet. Shape: (1176093, 7)\n",
      "Reading fhv_tripdata_2021-03.parquet...\n",
      "Successfully read fhv_tripdata_2021-03.parquet. Shape: (1302665, 7)\n",
      "Reading fhv_tripdata_2025-04.parquet...\n",
      "Successfully read fhv_tripdata_2025-04.parquet. Shape: (1699478, 7)\n",
      "Reading fhv_tripdata_2023-09.parquet...\n",
      "Successfully read fhv_tripdata_2023-09.parquet. Shape: (1293303, 7)\n",
      "Reading fhv_tripdata_2022-06.parquet...\n",
      "Successfully read fhv_tripdata_2022-06.parquet. Shape: (1195414, 7)\n",
      "Reading fhv_tripdata_2023-10.parquet...\n",
      "Successfully read fhv_tripdata_2023-10.parquet. Shape: (1628438, 7)\n",
      "Reading fhv_tripdata_2021-11.parquet...\n",
      "Successfully read fhv_tripdata_2021-11.parquet. Shape: (1228962, 7)\n",
      "Reading fhv_tripdata_2021-01.parquet...\n",
      "Successfully read fhv_tripdata_2021-01.parquet. Shape: (1154112, 7)\n",
      "Reading fhv_tripdata_2024-10.parquet...\n",
      "Successfully read fhv_tripdata_2024-10.parquet. Shape: (1421231, 7)\n",
      "Reading fhv_tripdata_2024-09.parquet...\n",
      "Successfully read fhv_tripdata_2024-09.parquet. Shape: (1718375, 7)\n",
      "Reading fhv_tripdata_2022-04.parquet...\n",
      "Successfully read fhv_tripdata_2022-04.parquet. Shape: (1246669, 7)\n",
      "Reading fhv_tripdata_2021-08.parquet...\n",
      "Successfully read fhv_tripdata_2021-08.parquet. Shape: (1203018, 7)\n",
      "Reading fhv_tripdata_2023-02.parquet...\n",
      "Successfully read fhv_tripdata_2023-02.parquet. Shape: (1110797, 7)\n",
      "Reading fhv_tripdata_2023-12.parquet...\n",
      "Successfully read fhv_tripdata_2023-12.parquet. Shape: (1376748, 7)\n",
      "Reading fhv_tripdata_2024-08.parquet...\n",
      "Successfully read fhv_tripdata_2024-08.parquet. Shape: (1484471, 7)\n",
      "Reading fhv_tripdata_2022-05.parquet...\n",
      "Successfully read fhv_tripdata_2022-05.parquet. Shape: (1255828, 7)\n",
      "Reading fhv_tripdata_2021-09.parquet...\n",
      "Successfully read fhv_tripdata_2021-09.parquet. Shape: (1179412, 7)\n",
      "Reading fhv_tripdata_2023-03.parquet...\n",
      "Successfully read fhv_tripdata_2023-03.parquet. Shape: (1328242, 7)\n",
      "Reading fhv_tripdata_2021-10.parquet...\n",
      "Successfully read fhv_tripdata_2021-10.parquet. Shape: (1277393, 7)\n",
      "Reading fhv_tripdata_2024-11.parquet...\n",
      "Successfully read fhv_tripdata_2024-11.parquet. Shape: (1591082, 7)\n",
      "Reading fhv_tripdata_2024-01.parquet...\n",
      "Successfully read fhv_tripdata_2024-01.parquet. Shape: (1290116, 7)\n",
      "\n",
      "--- All files combined ---\n",
      "Total rows in combined DataFrame: 70165194\n",
      "Total columns in combined DataFrame: 7\n",
      "\n",
      "--- First 5 rows of combined data ---\n",
      "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
      "0               B00014 2025-02-01 00:00:00 2025-02-01 00:33:00           NaN   \n",
      "1               B00053 2025-02-01 00:24:00 2025-02-01 00:39:00           NaN   \n",
      "2               B00112 2025-02-01 00:15:13 2025-02-01 00:22:34           NaN   \n",
      "3               B00112 2025-02-01 00:28:02 2025-02-01 00:35:54           NaN   \n",
      "4               B00221 2025-02-01 00:12:26 2025-02-01 00:18:11           NaN   \n",
      "\n",
      "   DOlocationID SR_Flag Affiliated_base_number  \n",
      "0           NaN     NaN                 B00014  \n",
      "1           NaN     NaN                 B00053  \n",
      "2          14.0     NaN                 B00112  \n",
      "3          14.0     NaN                 B00112  \n",
      "4         167.0     NaN                 B00221  \n",
      "\n",
      "--- Info of combined data ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70165194 entries, 0 to 70165193\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   dispatching_base_num    object        \n",
      " 1   pickup_datetime         datetime64[us]\n",
      " 2   dropOff_datetime        datetime64[us]\n",
      " 3   PUlocationID            float64       \n",
      " 4   DOlocationID            float64       \n",
      " 5   SR_Flag                 object        \n",
      " 6   Affiliated_base_number  object        \n",
      "dtypes: datetime64[us](2), float64(2), object(3)\n",
      "memory usage: 3.7+ GB\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "data_directory = '/Users/zuminchen/Desktop/Summer Research Project/FHV_Data'\n",
    "\n",
    "all_months_dfs = []\n",
    "\n",
    "# Loop through files in the specified directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    # Check if the filename matches the pattern: fhv_tripdata_YYYY-MM.parquet\n",
    "    if filename.startswith('fhv_tripdata_') and filename.endswith('.parquet'):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"Reading {filename}...\")\n",
    "        try:\n",
    "            # Read the parquet file into a PyArrow Table, then convert to Pandas\n",
    "            table = pq.read_table(file_path)\n",
    "            df = table.to_pandas()\n",
    "            all_months_dfs.append(df)\n",
    "            print(f\"Successfully read {filename}. Shape: {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if all_months_dfs:\n",
    "    combined_trips_df = pd.concat(all_months_dfs, ignore_index=True)\n",
    "    print(\"\\n--- All files combined ---\")\n",
    "    print(f\"Total rows in combined DataFrame: {combined_trips_df.shape[0]}\")\n",
    "    print(f\"Total columns in combined DataFrame: {combined_trips_df.shape[1]}\")\n",
    "    print(\"\\n--- First 5 rows of combined data ---\")\n",
    "    print(combined_trips_df.head())\n",
    "    print(\"\\n--- Info of combined data ---\")\n",
    "    combined_trips_df.info()\n",
    "\n",
    "else:\n",
    "    print(\"No Parquet files found or read successfully in the specified directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd386a23",
   "metadata": {},
   "source": [
    "Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba323ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num             0\n",
       "pickup_datetime                  0\n",
       "dropOff_datetime                 0\n",
       "PUlocationID              55565871\n",
       "DOlocationID              11452809\n",
       "SR_Flag                   70165194\n",
       "Affiliated_base_number         885\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_trips_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80387724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a numeric type first, coercing errors to NaN, then fill and convert to int\n",
    "combined_trips_df['SR_Flag'] = pd.to_numeric(combined_trips_df['SR_Flag'], errors='coerce').fillna(0).astype(int)\n",
    "# Drop rows with empty values of locationID\n",
    "combined_trips_df.dropna(subset=['PUlocationID', 'DOlocationID'], inplace=True)\n",
    "# Fill with 'unknown' for Affiliated_base_number\n",
    "combined_trips_df['Affiliated_base_number'] = combined_trips_df['Affiliated_base_number'].fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a138a7",
   "metadata": {},
   "source": [
    "Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38073ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert base numbers to categorical type for efficiency\n",
    "combined_trips_df['dispatching_base_num'] = combined_trips_df['dispatching_base_num'].astype('category')\n",
    "combined_trips_df['Affiliated_base_number'] = combined_trips_df['Affiliated_base_number'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02501e14",
   "metadata": {},
   "source": [
    "Time Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72575163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip Duration Feature Engineering\n",
    "time_difference = combined_trips_df['dropOff_datetime'] - combined_trips_df['pickup_datetime']\n",
    "combined_trips_df['trip_duration_minutes'] = time_difference.dt.total_seconds() / 60\n",
    "# Extracting Time-Based Features\n",
    "combined_trips_df['hour_of_day'] = combined_trips_df['pickup_datetime'].dt.hour\n",
    "combined_trips_df['day_of_week'] = combined_trips_df['pickup_datetime'].dt.dayofweek # Monday=0, Sunday=6\n",
    "combined_trips_df['day_of_month'] = combined_trips_df['pickup_datetime'].dt.day\n",
    "combined_trips_df['month'] = combined_trips_df['pickup_datetime'].dt.month\n",
    "combined_trips_df['year'] = combined_trips_df['pickup_datetime'].dt.year\n",
    "combined_trips_df['is_weekend'] = combined_trips_df['pickup_datetime'].dt.weekday >= 5 # Boolean: True for Sat/Sun\n",
    "combined_trips_df['quarter'] = combined_trips_df['pickup_datetime'].dt.quarter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3510989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape after filtering durations: (14100256, 15)\n"
     ]
    }
   ],
   "source": [
    "# Filtering out unrealistic trip_duration_minutes\n",
    "combined_trips_df = combined_trips_df[\n",
    "      (combined_trips_df['trip_duration_minutes'] > 0) &\n",
    "      (combined_trips_df['trip_duration_minutes'] <= 240)].copy() \n",
    "# .copy() to avoid SettingWithCopyWarning\n",
    "print(f\"DataFrame shape after filtering durations: {combined_trips_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f92d79",
   "metadata": {},
   "source": [
    "Introduce Earlier Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77209997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading gkne-dk5s.csv into a DataFrame...\n",
      "Successfully read gkne-dk5s.csv. Shape: (1000000, 19)\n",
      "\n",
      "--- First 5 rows of the new DataFrame (first 10 columns) ---\n",
      "  vendor_id          pickup_datetime         dropoff_datetime  \\\n",
      "0       CMT  2014-10-04T23:04:31.000  2014-10-04T23:13:49.000   \n",
      "1       CMT  2014-03-25T23:15:28.000  2014-03-25T23:23:42.000   \n",
      "2       VTS  2014-12-09T20:39:00.000  2014-12-09T20:39:00.000   \n",
      "3       CMT  2014-06-08T10:10:21.000  2014-06-08T10:11:56.000   \n",
      "4       CMT  2014-07-28T00:02:14.000  2014-07-28T00:07:44.000   \n",
      "\n",
      "   passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
      "0                1            1.3        -73.991092        40.717533   \n",
      "1                1            1.3        -73.967118        40.756929   \n",
      "2                1            0.0        -73.991015        40.760652   \n",
      "3                1            0.5        -73.978248        40.783308   \n",
      "4                1            1.1        -73.984075        40.725214   \n",
      "\n",
      "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  \n",
      "0                  N         -74.003639         40.729226  \n",
      "1                  N         -73.985774         40.766026  \n",
      "2                NaN         -73.991015         40.760652  \n",
      "3                  N         -73.971652         40.787987  \n",
      "4                  N         -74.001896         40.726288  \n",
      "\n",
      "--- Info of the new DataFrame (first 10 columns) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   vendor_id           1000000 non-null  object \n",
      " 1   pickup_datetime     1000000 non-null  object \n",
      " 2   dropoff_datetime    1000000 non-null  object \n",
      " 3   passenger_count     1000000 non-null  int64  \n",
      " 4   trip_distance       1000000 non-null  float64\n",
      " 5   pickup_longitude    1000000 non-null  float64\n",
      " 6   pickup_latitude     1000000 non-null  float64\n",
      " 7   store_and_fwd_flag  488768 non-null   object \n",
      " 8   dropoff_longitude   1000000 non-null  float64\n",
      " 9   dropoff_latitude    1000000 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(4)\n",
      "memory usage: 76.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = '/Users/zuminchen/Desktop/Summer Research Project/FHV_Data'\n",
    "new_csv_file = 'gkne-dk5s.csv'\n",
    "file_path = os.path.join(data_directory, new_csv_file)\n",
    "\n",
    "# Load the new data into a DataFrame\n",
    "try:\n",
    "    print(f\"Reading {new_csv_file} into a DataFrame...\")\n",
    "    # Using low_memory=False to avoid DtypeWarning for mixed-type columns\n",
    "    new_trips_df = pd.read_csv(file_path, low_memory=False)\n",
    "    print(f\"Successfully read {new_csv_file}. Shape: {new_trips_df.shape}\")\n",
    "    \n",
    "    print(\"\\n--- First 5 rows of the new DataFrame (first 10 columns) ---\")\n",
    "    # Use .iloc[:, :10] to select all rows and the first 10 columns\n",
    "    print(new_trips_df.iloc[:5, :10])\n",
    "\n",
    "    print(\"\\n--- Info of the new DataFrame (first 10 columns) ---\")\n",
    "    new_trips_df.iloc[:, :10].info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{new_csv_file}' was not found at '{file_path}'.\")\n",
    "    print(\"Please ensure the file exists and the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b942505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi zones are ready with WGS84 CRS.\n",
      "Pickup locations GeoDataFrame created.\n",
      "Dropoff locations GeoDataFrame created.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# A new GeoDataFrame 'zones_latlon' will be created with the correct CRS (WGS84 Lat/Lon).\n",
    "if taxi_zones.crs is None or taxi_zones.crs.to_epsg() != 4326:\n",
    "    zones_latlon = taxi_zones.to_crs(epsg=4326)\n",
    "else:\n",
    "    zones_latlon = taxi_zones.copy()\n",
    "print(\"Taxi zones are ready with WGS84 CRS.\")\n",
    "\n",
    "# Create a 'geometry' column of Shapely Point objects.\n",
    "# It's crucial to match the order: Point(longitude, latitude)\n",
    "pickup_geometry = [Point(xy) for xy in zip(new_trips_df['pickup_longitude'], new_trips_df['pickup_latitude'])]\n",
    "pickup_locations_gdf = gpd.GeoDataFrame(new_trips_df, geometry=pickup_geometry, crs=\"EPSG:4326\")\n",
    "print(\"Pickup locations GeoDataFrame created.\")\n",
    "\n",
    "dropoff_geometry = [Point(xy) for xy in zip(new_trips_df['dropoff_longitude'], new_trips_df['dropoff_latitude'])]\n",
    "dropoff_locations_gdf = gpd.GeoDataFrame(new_trips_df, geometry=dropoff_geometry, crs=\"EPSG:4326\")\n",
    "print(\"Dropoff locations GeoDataFrame created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d685e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUlocationID added to trips.\n",
      "DOlocationID added to trips.\n",
      "  vendor_id          pickup_datetime         dropoff_datetime  \\\n",
      "0       CMT  2014-10-04T23:04:31.000  2014-10-04T23:13:49.000   \n",
      "1       CMT  2014-03-25T23:15:28.000  2014-03-25T23:23:42.000   \n",
      "2       VTS  2014-12-09T20:39:00.000  2014-12-09T20:39:00.000   \n",
      "3       CMT  2014-06-08T10:10:21.000  2014-06-08T10:11:56.000   \n",
      "4       CMT  2014-07-28T00:02:14.000  2014-07-28T00:07:44.000   \n",
      "\n",
      "   passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
      "0                1            1.3        -73.991092        40.717533   \n",
      "1                1            1.3        -73.967118        40.756929   \n",
      "2                1            0.0        -73.991015        40.760652   \n",
      "3                1            0.5        -73.978248        40.783308   \n",
      "4                1            1.1        -73.984075        40.725214   \n",
      "\n",
      "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  ... fare_amount  \\\n",
      "0                  N         -74.003639         40.729226  ...         7.5   \n",
      "1                  N         -73.985774         40.766026  ...         7.5   \n",
      "2                NaN         -73.991015         40.760652  ...        15.0   \n",
      "3                  N         -73.971652         40.787987  ...         3.5   \n",
      "4                  N         -74.001896         40.726288  ...         6.5   \n",
      "\n",
      "   mta_tax  tip_amount  tolls_amount  total_amount  imp_surcharge  extra  \\\n",
      "0      0.5         1.0           0.0           9.5            0.5    NaN   \n",
      "1      0.5         0.0           0.0           8.5            0.5    NaN   \n",
      "2      0.5         4.5           0.0          20.0            0.0    NaN   \n",
      "3      0.5         0.5           0.0           4.5            0.0    NaN   \n",
      "4      0.5         0.0           0.0           7.5            0.5    NaN   \n",
      "\n",
      "   rate_code  PUlocationID  DOlocationID  \n",
      "0          1         148.0         249.0  \n",
      "1          1         229.0          48.0  \n",
      "2          5          48.0          48.0  \n",
      "3          1         239.0         238.0  \n",
      "4          1          79.0         211.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Trips with no matching pickup zone: 21110\n",
      "Trips with no matching dropoff zone: 22876\n"
     ]
    }
   ],
   "source": [
    "# Use 'left' join to keep all trips from new_trips_df, even if a point falls outside a zone.\n",
    "# The 'op' (operation) 'within' checks if a point is inside a polygon.\n",
    "new_trips_with_pu = gpd.sjoin(pickup_locations_gdf, zones_latlon[['LocationID', 'geometry']], how=\"left\", predicate='within')\n",
    "# Rename the new LocationID column to reflect it's the pickup zone\n",
    "new_trips_with_pu = new_trips_with_pu.rename(columns={'LocationID': 'PUlocationID'})\n",
    "print(\"PUlocationID added to trips.\")\n",
    "\n",
    "# Join the dropoff locations GeoDataFrame with the taxi zones.\n",
    "new_trips_with_do = gpd.sjoin(dropoff_locations_gdf, zones_latlon[['LocationID', 'geometry']], how=\"left\", predicate='within')\n",
    "# Rename the new LocationID column to reflect it's the dropoff zone\n",
    "new_trips_with_do = new_trips_with_do.rename(columns={'LocationID': 'DOlocationID'})\n",
    "print(\"DOlocationID added to trips.\")\n",
    "\n",
    "# Combine the results into a single DataFrame\n",
    "new_trips_df['PUlocationID'] = new_trips_with_pu['PUlocationID'].values\n",
    "new_trips_df['DOlocationID'] = new_trips_with_do['DOlocationID'].values\n",
    "\n",
    "# Display the head of the new DataFrame to confirm the new columns exist.\n",
    "print(new_trips_df.head())\n",
    "\n",
    "# The 'PUlocationID' and 'DOlocationID' columns will have NaN values if a point fell outside a zone.\n",
    "print(f\"\\nTrips with no matching pickup zone: {new_trips_df['PUlocationID'].isnull().sum()}\")\n",
    "print(f\"Trips with no matching dropoff zone: {new_trips_df['DOlocationID'].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82435dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped 24964 trips that had no matching taxi zone.\n",
      "Remaining trips in the DataFrame: 975036\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with no matching zones\n",
    "initial_rows = new_trips_df.shape[0]\n",
    "new_trips_df.dropna(subset=['PUlocationID', 'DOlocationID'], inplace=True)\n",
    "final_rows = new_trips_df.shape[0]\n",
    "\n",
    "print(f\"\\nDropped {initial_rows - final_rows} trips that had no matching taxi zone.\")\n",
    "print(f\"Remaining trips in the DataFrame: {final_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf57676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14100256 entries, 40 to 70165175\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   dispatching_base_num    category      \n",
      " 1   pickup_datetime         datetime64[us]\n",
      " 2   dropOff_datetime        datetime64[us]\n",
      " 3   PUlocationID            float64       \n",
      " 4   DOlocationID            float64       \n",
      " 5   SR_Flag                 int64         \n",
      " 6   Affiliated_base_number  category      \n",
      " 7   trip_duration_minutes   float64       \n",
      " 8   hour_of_day             int32         \n",
      " 9   day_of_week             int32         \n",
      " 10  day_of_month            int32         \n",
      " 11  month                   int32         \n",
      " 12  year                    int32         \n",
      " 13  is_weekend              bool          \n",
      " 14  quarter                 int32         \n",
      "dtypes: bool(1), category(2), datetime64[us](2), float64(3), int32(6), int64(1)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "combined_trips_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91409fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 975036 entries, 0 to 999999\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   vendor_id           975036 non-null  object \n",
      " 1   pickup_datetime     975036 non-null  object \n",
      " 2   dropoff_datetime    975036 non-null  object \n",
      " 3   passenger_count     975036 non-null  int64  \n",
      " 4   trip_distance       975036 non-null  float64\n",
      " 5   pickup_longitude    975036 non-null  float64\n",
      " 6   pickup_latitude     975036 non-null  float64\n",
      " 7   store_and_fwd_flag  474866 non-null  object \n",
      " 8   dropoff_longitude   975036 non-null  float64\n",
      " 9   dropoff_latitude    975036 non-null  float64\n",
      " 10  payment_type        975036 non-null  object \n",
      " 11  fare_amount         975036 non-null  float64\n",
      " 12  mta_tax             975036 non-null  float64\n",
      " 13  tip_amount          975036 non-null  float64\n",
      " 14  tolls_amount        975036 non-null  float64\n",
      " 15  total_amount        975036 non-null  float64\n",
      " 16  imp_surcharge       975036 non-null  float64\n",
      " 17  extra               0 non-null       float64\n",
      " 18  rate_code           975036 non-null  int64  \n",
      " 19  PUlocationID        975036 non-null  float64\n",
      " 20  DOlocationID        975036 non-null  float64\n",
      "dtypes: float64(14), int64(2), object(5)\n",
      "memory usage: 163.7+ MB\n"
     ]
    }
   ],
   "source": [
    "new_trips_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977128c",
   "metadata": {},
   "source": [
    "Decide the Specific Zone to Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2277a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Busiest Pickup Zones:\n",
      "     PUlocationID  trip_count\n",
      "215         237.0       35670\n",
      "145         161.0       33698\n",
      "71           79.0       33442\n",
      "146         162.0       32798\n",
      "154         170.0       32383\n",
      "208         230.0       32005\n",
      "212         234.0       31790\n",
      "214         236.0       31773\n",
      "41           48.0       31430\n",
      "168         186.0       30091\n"
     ]
    }
   ],
   "source": [
    "# Group by PUlocationID and count the number of trips for each zone\n",
    "busiest_pickup_zones = new_trips_df.groupby('PUlocationID').size().reset_index(name='trip_count')\n",
    "\n",
    "# Sort in descending order to find the top 10 busiest zones\n",
    "top_10_pickup_zones = busiest_pickup_zones.sort_values(by='trip_count', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Busiest Pickup Zones:\")\n",
    "print(top_10_pickup_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e5851",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29cb5646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'npz/new_trips_with_zones.npz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import npz_io\n",
    "\n",
    "outdir = Path(\"npz\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Uncompressed .npz\n",
    "npz_io.save_df_npz(outdir / \"combined_trips_clean.npz\", combined_trips_df, compress=False)\n",
    "npz_io.save_df_npz(outdir / \"new_trips_with_zones.npz\", new_trips_df, compress=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
